{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "6e8bf2f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e8bf2f4",
        "outputId": "fbbf1551-68cc-4d9f-c840-a8edc7ec6eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  article_id  conversation_id  turn_id   speaker  \\\n",
            "0   0          35                1        0  Person 1   \n",
            "1   1          35                1        1  Person 2   \n",
            "2   2          35                1        2  Person 1   \n",
            "3   3          35                1        3  Person 2   \n",
            "4   4          35                1        4  Person 1   \n",
            "\n",
            "                                                text person_id_1 person_id_2  \\\n",
            "0              what did you think about this article        p019        p012   \n",
            "1  It's definitely really sad to read, considerin...        p019        p012   \n",
            "2  I think it's super sad... they seem to never c...        p019        p012   \n",
            "3  I can't imagine just living in an area that is...        p019        p012   \n",
            "4  Me too.. I also can't imagine living in the po...        p019        p012   \n",
            "\n",
            "   Emotion  EmotionalPolarity  Empathy  SelfDisclosure  \n",
            "0        1                  1        1          1.0000  \n",
            "1        3                  2        4          2.0000  \n",
            "2        4                  2        5          2.6667  \n",
            "3        3                  2        5          3.3333  \n",
            "4        3                  2        5          3.0000  \n"
          ]
        }
      ],
      "source": [
        "# import gensim.downloader as api\n",
        "# from gensim.models import KeyedVectors\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, classification_report\n",
        "\n",
        "# fx. used to lowercase the sentence and strip of punctuation\n",
        "def cleanText(train_text):\n",
        "  return_list = []\n",
        "  for i in range(len(train_text)):\n",
        "    # strip of punctuation and lowercase\n",
        "    cleaned_string = re.sub(r'[^\\w\\s]', '', train_text[i].lower())\n",
        "    # strip and append to list\n",
        "    return_list.append(cleaned_string.strip())\n",
        "  return return_list\n",
        "\n",
        "# tokenize each sentence to words [\"this\", \"is\", \"sample\"]\n",
        "def tokenize(text):\n",
        "  word_list = []\n",
        "  for sentence in text:\n",
        "    word_list.append(sentence.split())\n",
        "  return word_list\n",
        "\n",
        "# create a vocabulary set of word -> index to be used later\n",
        "def buildVocab(sentence_list, dict):\n",
        "  count = 2\n",
        "  for sentence in sentence_list:\n",
        "    for word in sentence:\n",
        "      if word not in dict:\n",
        "        dict[word] = count\n",
        "        count = count + 1\n",
        "  return dict\n",
        "\n",
        "# this fx. converts each setence in the data set to indices [\"this\", \"is\", \"sample\"] -> [2,4,5]\n",
        "def numerize(data_set, vocab_set):\n",
        "  return_set = []\n",
        "  for sentence in data_set:\n",
        "    sentence_list = []\n",
        "    for word in sentence:\n",
        "      if word in vocab_set:\n",
        "        sentence_list.append(vocab_set[word])\n",
        "      else:\n",
        "        sentence_list.append(vocab_set[\"<UNK>\"])\n",
        "    return_set.append(sentence_list)\n",
        "\n",
        "  # print(return_set)\n",
        "  return return_set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "  # loading the datasets\n",
        "  train_df = pd.read_csv('https://raw.githubusercontent.com/QUANHONGLE/CS421-emotion-prediction/main/P1_data/trac2_CONVT_train.csv')\n",
        "  test_df =  pd.read_csv('https://raw.githubusercontent.com/QUANHONGLE/CS421-emotion-prediction/main/P1_data/trac2_CONVT_test.csv', on_bad_lines='skip')\n",
        "  dev_df =  pd.read_csv('https://raw.githubusercontent.com/QUANHONGLE/CS421-emotion-prediction/main/P1_data/trac2_CONVT_dev.csv', on_bad_lines='skip')\n",
        "\n",
        "\n",
        "  print(train_df.head())\n",
        "  # print(\"\\n\")\n",
        "  # print(\"TEST:\")\n",
        "  # print(test_df.head())\n",
        "  # print(\"\\n\")\n",
        "  # print(\"DEV\")\n",
        "  # print(dev_df.head())\n",
        "\n",
        "  # split the data set into texts for preprocessing\n",
        "  train_text = train_df[\"text\"].astype(str).tolist()\n",
        "  test_text = test_df[\"text\"].astype(str).tolist()\n",
        "  dev_text = dev_df[\"text\"].astype(str).tolist()\n",
        "\n",
        "  # train id,  emotion, polarity, and empathy\n",
        "  train_id = train_df[\"id\"].astype(int).tolist()\n",
        "  train_emotion = train_df[\"Emotion\"].astype(float).tolist()\n",
        "  train_polarity = train_df[\"EmotionalPolarity\"].astype(int).tolist()\n",
        "  train_empathy = train_df[\"Empathy\"].astype(float).tolist()\n",
        "\n",
        "\n",
        "\n",
        "  cleaned_train = cleanText(train_text)\n",
        "  tokenized_train_list = tokenize(cleaned_train)\n",
        "  # print(tokenized_sentence_list)\n",
        "\n",
        "  # processing the test set\n",
        "  cleaned_test = cleanText(test_text)\n",
        "  tokenized_test_list = tokenize(cleaned_test)\n",
        "  # processing the dev set\n",
        "  cleaned_dev = cleanText(dev_text)\n",
        "  tokenized_dev_list = tokenize(cleaned_dev)\n",
        "\n",
        "  # building a vocab set for normalization\n",
        "  vocabulary_set = {\"<PAD>\" : 0 , \"<UNK>\": 1}\n",
        "  buildVocab(tokenized_train_list, vocabulary_set)\n",
        "\n",
        "  train_text_to_index = numerize(tokenized_train_list, vocabulary_set)\n",
        "  test_text_to_index = numerize(tokenized_test_list, vocabulary_set)\n",
        "  dev_text_to_index = numerize(tokenized_dev_list, vocabulary_set)\n",
        "\n",
        "\n",
        "\n",
        "  # print(train_text_to_index)\n",
        "  # print(test_text_to_index)\n",
        "  # print(dev_text_to_index)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ax9zsCk1LHOt"
      },
      "id": "Ax9zsCk1LHOt",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}